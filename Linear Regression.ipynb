{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "from algora.base import BaseEstimator\n",
    "from algora.metrics.metrics import mean_squared_error, binary_crossentropy\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "class BasicRegression(BaseEstimator):\n",
    "    def __init__(self, lr=0.001, penalty='None', C=0.01, tolerance=0.0001, max_iters=1000):\n",
    "        \"\"\"Basic class for implementing continuous regression estimators which\n",
    "        are trained with gradient descent optimization on their particular loss\n",
    "        function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float, default 0.001\n",
    "            Learning rate.\n",
    "        penalty : str, {'l1', 'l2', None'}, default None\n",
    "            Regularization function name.\n",
    "        C : float, default 0.01\n",
    "            The regularization coefficient.\n",
    "        tolerance : float, default 0.0001\n",
    "            If the gradient descent updates are smaller than `tolerance`, then\n",
    "            stop optimization process.\n",
    "        max_iters : int, default 10000\n",
    "            The maximum number of iterations.\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.penalty = penalty\n",
    "        self.tolerance = tolerance\n",
    "        self.lr = lr\n",
    "        self.max_iters = max_iters\n",
    "        self.errors = []\n",
    "        self.theta = []\n",
    "        self.n_samples, self.n_features = None, None\n",
    "        self.cost_func = None\n",
    "\n",
    "    def _loss(self, w):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def init_cost(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _add_penalty(self, loss, w):\n",
    "        \"\"\"Apply regularization to the loss.\"\"\"\n",
    "        if self.penalty == \"l1\":\n",
    "            loss += self.C * np.abs(w[:-1]).sum()\n",
    "        elif self.penalty == \"l2\":\n",
    "            loss += (0.5 * self.C) * (w[:-1] ** 2).mean()\n",
    "        return loss\n",
    "\n",
    "    def _cost(self, X, y, theta):\n",
    "        prediction = X.dot(theta)\n",
    "        error = self.cost_func(y, prediction)\n",
    "        return error\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._setup_input(X, y)\n",
    "        self.init_cost()\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "        # Initialize weights + bias term\n",
    "        self.theta = np.random.normal(size=(self.n_features + 1), scale=0.5)\n",
    "\n",
    "        # Add an intercept column\n",
    "        self.X = self._add_intercept(self.X)\n",
    "\n",
    "        self._train()\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_intercept(X):\n",
    "        b = np.ones([X.shape[0], 1])\n",
    "        return np.concatenate([b, X], axis=1)\n",
    "\n",
    "    def _train(self):\n",
    "        self.theta, self.errors = self._gradient_descent()\n",
    "        logging.info(' Theta: %s' % self.theta.flatten())\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        X = self._add_intercept(X)\n",
    "        return X.dot(self.theta)\n",
    "\n",
    "    def _gradient_descent(self):\n",
    "        theta = self.theta\n",
    "        errors = [self._cost(self.X, self.y, theta)]\n",
    "\n",
    "        for i in range(1, self.max_iters + 1):\n",
    "            # Get derivative of the loss function\n",
    "            cost_d = grad(self._loss)\n",
    "            # Calculate gradient and update theta\n",
    "            delta = cost_d(theta)\n",
    "            theta -= self.lr * delta\n",
    "\n",
    "            errors.append(self._cost(self.X, self.y, theta))\n",
    "            logging.info('Iteration %s, error %s' % (i, errors[i]))\n",
    "\n",
    "            error_diff = np.linalg.norm(errors[i - 1] - errors[i])\n",
    "            if error_diff < self.tolerance:\n",
    "                logging.info('Convergence has reached.')\n",
    "                break\n",
    "        return theta, errors\n",
    "\n",
    "\n",
    "class LinearRegression(BasicRegression):\n",
    "    \"\"\"Linear regression with gradient descent optimizer.\"\"\"\n",
    "\n",
    "    def _loss(self, w):\n",
    "        loss = self.cost_func(self.y, np.dot(self.X, w))\n",
    "        return self._add_penalty(loss, w)\n",
    "\n",
    "    def init_cost(self):\n",
    "        self.cost_func = mean_squared_error\n",
    "\n",
    "\n",
    "class LogisticRegression(BasicRegression):\n",
    "    \"\"\"Binary logistic regression with gradient descent optimizer.\"\"\"\n",
    "\n",
    "    def init_cost(self):\n",
    "        self.cost_func = binary_crossentropy\n",
    "\n",
    "    def _loss(self, w):\n",
    "        loss = self.cost_func(self.y, self.sigmoid(np.dot(self.X, w)))\n",
    "        return self._add_penalty(loss, w)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 0.5 * (np.tanh(x) + 1)\n",
    "\n",
    "    def _predict(self, X=None):\n",
    "        X = self._add_intercept(X)\n",
    "        return self.sigmoid(X.dot(self.theta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression mse 0.014299757949575644\n",
      "classification accuracy 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharth/miniconda3/lib/python3.6/site-packages/autograd/tracer.py:48: RuntimeWarning: overflow encountered in cosh\n",
      "  return f_raw(*args, **kwargs)\n",
      "/Users/siddharth/miniconda3/lib/python3.6/site-packages/autograd/numpy/numpy_vjps.py:88: RuntimeWarning: overflow encountered in square\n",
      "  defvjp(anp.tanh,   lambda ans, x : lambda g: g / anp.cosh(x) **2)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from algora.linear_models import LinearRegression, LogisticRegression\n",
    "from algora.metrics.metrics import mean_squared_error, accuracy\n",
    "\n",
    "# Change to DEBUG to see convergence\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "def regression():\n",
    "    # Generate a random regression problem\n",
    "    X, y = make_regression(n_samples=10000, n_features=100,\n",
    "                           n_informative=75, n_targets=1, noise=0.05,\n",
    "                           random_state=1111, bias=0.5)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
    "                                                        random_state=1111)\n",
    "\n",
    "    model = LinearRegression(lr=0.01, max_iters=2000, penalty='l2', C=0.03)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print('regression mse', mean_squared_error(y_test, predictions))\n",
    "\n",
    "\n",
    "def classification():\n",
    "    # Generate a random binary classification problem.\n",
    "    X, y = make_classification(n_samples=1000, n_features=100,\n",
    "                               n_informative=75, random_state=1111,\n",
    "                               n_classes=2, class_sep=2.5, )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n",
    "                                                        random_state=1111)\n",
    "\n",
    "    model = LogisticRegression(lr=0.01, max_iters=500, penalty='l1', C=0.01)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print('classification accuracy', accuracy(y_test, predictions))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    regression()\n",
    "    classification()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
